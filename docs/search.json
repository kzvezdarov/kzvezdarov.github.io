[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kiril Zvezdarov’s Blog",
    "section": "",
    "text": "Modeling Formula 1 Fantasy GP points with PyMC\n\n\n\n\n\n\n\n\n\n\n\n\nAug 31, 2023\n\n\nKiril Zvezdarov\n\n\n\n\n\n\n  \n\n\n\n\nPointer tagging with multiple tags\n\n\n\n\n\n\n\n\n\n\n\n\nDec 18, 2016\n\n\nKiril Zvezdarov\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/pointer-tagging/index.html",
    "href": "posts/pointer-tagging/index.html",
    "title": "Pointer tagging with multiple tags",
    "section": "",
    "text": "This is a short overview of pointer tagging in situations where more than one tags or multibit tags are needed. The implementation is in Rust, but it should be easy to understand even without knowledge of the language.\nThe snippet below is part of my attempt to implement a lockfree linked list, as described by Harris, with the additional optimizations by Fomitchev, Ruppert. The original Harris algorithm uses the least significant bit of the “successor” pointer in each node of the linked list as a deletion mark, to achieve a two phase removal - first, a node is logically deleted by tagging its successor pointer with the mark, and at a later point it is unlinked completely. The optimizations by Fomitchev and Ruppert add a second possible tag at the next least significant bit, as well as a backlink to a previous node, in order to shorten the length and amount of traversals of the linked list a process has to make. The new tag “flags” that the node after the current one is being deleted, and that the flagged node should not be marked until after the deletion of its successor is fully completed.\nThe two tags need to be manipulated individually when the algorithm is setting the metadata of a node, and (for convenience) as one chunk for when the actual pointer, clear of tags, is needed:\nconst MARK_BIT: usize = 1 &lt;&lt; 0;\n\nconst FLAG_BIT: usize = 1 &lt;&lt; 1;\n\nconst ALL_TAGS: usize = MARK_BIT + FLAG_BIT;\n\nfn tag_at&lt;T&gt;(ptr: *const T, tag: usize, value: bool) -&gt; *const T {\n    (ptr as usize & !tag | (tag * value as usize)) as *const T\n}\n\nfn is_tagged&lt;T&gt;(ptr: *const T, tag: usize) -&gt; bool {\n    (ptr as usize & tag) == tag\n}\nThe constants define the tag locations for the mark and flag, as well as the “mask” (for a lack of a better term) which covers all tags, so that they can be cleared in one go. The mask is just the sum of the tags that it needs to cover - since each tag bit is just an integer with only one of the bits set, e.g. 0b01c= for the least significant and 0b10 for the next least significant, their sum produces an integer whose set bits correspond to the location of all tags. This can be used for things more interesting than clearing the pointer - for example, it allows for multibit tags or storing (small) integer values, all in one function call.\nThe tagging logic is only slightly different from how it is usually implemented. First, the pointer and the inverse of the tag bit are and-ed, which results in an integer whose tag bit is unset. This in a sense isolates that specific tag, as the integer now is in a “clean” state with respect to it. The result is or-ed with the tag bit value, which will either set the tag bit or keep it unset (the tag value is just the tag bit or 0).\nTo illustrate, here is a contrived example:\n\nInitially, let ptr = 1010101100001011, let tag = 1 &lt;&lt; 1, and let value = false. Both tags are set.\nptr & !tag = 1010101100001011 & 1111111111111101 = 1010101100001001. Note that only the targeted tag is unset, otherwise the pointer is the same. It is clean with respect to the target tag.\n(ptr & !tag) | 0 = 1010101100001001 | 0 = 1010101100001001\n\nChecking if a tag is set is done just by =and=-ing the pointer and the tag bits and checking that that results in the tag bits. Lastly, since the tag can be multiple bits, clearing the pointer for regular usage can be done by just tag_at(ptr, ALL_TAGS, false).\nNote that working with tagged pointers is tricky and dangerous, as accidentally dereferencing an unclean pointer will lead to Fun. In Rust, taking the raw pointer produced by the tagging function and dereferencing it would be an unsafe operation, which forces the implementor to take special note of where and how it is used."
  },
  {
    "objectID": "posts/fantasy-gp-mcmc/fantasy-gp-mcmc.html",
    "href": "posts/fantasy-gp-mcmc/fantasy-gp-mcmc.html",
    "title": "Modeling Formula 1 Fantasy GP points with PyMC",
    "section": "",
    "text": "Code\n%matplotlib inline\n\nimport json\nimport os\nimport warnings\nfrom enum import Enum\nfrom pathlib import Path\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nimport arviz as az\nimport numpy as np\nimport pandas as pd\nimport preliz as pz\nimport pymc as pm\nimport requests\nimport seaborn as sns\nimport tqdm\nfrom matplotlib import pyplot as plt\nfrom scipy import stats\n\nsns.set_theme()\nsns.set_style(style=\"darkgrid\", rc={\"axes.facecolor\": \".9\", \"grid.color\": \".8\"})\nsns.set_palette(palette=\"deep\")\nsns_c = sns.color_palette(palette=\"deep\")\n\nplt.rcParams[\"figure.figsize\"] = [10, 7]\n\naz.style.use(\"arviz-docgrid\")\n\n\nWARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\nLike many others I got into Formula 1 after binging through Drive to Survive. After enduring a nail biting (and ultimately bitterly disappointing) 2021 season, and attending the 2022 Canadian Grand Prix, I organized a small fantasy league for 2023, hosted on Fantasy GP.\nFantasy GP’s rules give each player a set budget to build a team consisting of 3 drivers and 3 constructor. During the season, drivers and constructors are awarded points for position finished during at the race (matching the actual world championship points awarded), as well as for bonuses such as beating their teammate and gaining positions during the race. This effectively brings lower midfied and backmarker drivers into the mix, since they can generate points from performance relative to their rivals.\nThis notebook will attempt to model individual driver and constructor performance in terms of Fantasy GP points earned, in order to provide a slightly more quantifiable basis for team construction. We focus on the most fundamental question - how many points per race is a given driver/constructor likely to earn. To answer this, we will create and evaluate a simple Bayesian linear model using the probabilistic programming framework PyMC."
  },
  {
    "objectID": "posts/fantasy-gp-mcmc/fantasy-gp-mcmc.html#data-acquisition",
    "href": "posts/fantasy-gp-mcmc/fantasy-gp-mcmc.html#data-acquisition",
    "title": "Modeling Formula 1 Fantasy GP points with PyMC",
    "section": "1 Data acquisition",
    "text": "1 Data acquisition\nWe start by acquiring data for the 2023 season. The Ergast API hosts statistics for Formula 1 races from the 1950s to the present day and exposes convenient endpoints for querying both qualifying and grand prix results.\nFirst, we define some convenience functions to query the raw json data:\n\nAPI = \"http://ergast.com/api/f1\"\n\n\n\nCode\nDATASET_PATH = {\"gp\": \"results\", \"quali\": \"qualifying\"}\n\n\ndef get_dataset(dataset: str, year: int, limit: int = 10000) -&gt; dict:\n    path = DATASET_PATH[dataset]\n    url = f\"{API}/{year}/{path}.json?limit={limit}\"\n\n    response = requests.get(url)\n    response.raise_for_status()\n\n    return response.json()\n\n\ndef load_dataset(dataset: str, year: int) -&gt; dict:\n    results_path = Path(f\".data/{year}_{dataset}_results.json\")\n\n    if results_path.exists():\n        with results_path.open() as f:\n            results = json.loads(f.read())\n    else:\n        results = get_dataset(dataset, year)\n\n        if not os.path.isdir(\".data\"):\n            os.makedirs(\".data\")\n\n        with results_path.open(\"w\") as f:\n            f.write(json.dumps(results))\n\n    return results\n\n\n\ngp_data = load_dataset(\"gp\", 2023)\nquali_data = load_dataset(\"quali\", 2023)\n\nNext, we unnest it into a row-wise format and load it as a Pandas dataframe for ease of use:\n\n\nCode\ndef gp_results_to_dataframe(json_data):\n    data = {}\n    for gp in json_data[\"MRData\"][\"RaceTable\"][\"Races\"]:\n        for result in gp[\"Results\"]:\n            data.setdefault(\"circuit\", []).append(gp[\"Circuit\"][\"circuitId\"])\n            data.setdefault(\"driver\", []).append(result[\"Driver\"][\"code\"])\n            data.setdefault(\"round\", []).append(gp[\"round\"])\n            data.setdefault(\"constructor\", []).append(\n                result[\"Constructor\"][\"constructorId\"]\n            )\n\n            data.setdefault(\"grid\", []).append(result[\"grid\"])\n            data.setdefault(\"position\", []).append(result[\"position\"])\n            data.setdefault(\"points\", []).append(result[\"points\"])\n            data.setdefault(\"fastest_lap\", []).append(\n                result.get(\"FastestLap\", {}).get(\"rank\")\n            )\n            data.setdefault(\"status\", []).append(result[\"status\"])\n\n    return (\n        pd.DataFrame(data)\n        .astype(\n            {\n                \"circuit\": \"category\",\n                \"driver\": \"category\",\n                \"round\": \"int\",\n                \"constructor\": \"category\",\n                \"status\": \"category\",\n                \"grid\": \"Int64\",\n                \"position\": \"Int64\",\n                \"points\": \"float32\",\n                \"fastest_lap\": \"Int64\",\n            }\n        )\n        .set_index([\"circuit\", \"driver\"])\n    )\n\n\ndef quali_results_to_dataframe(json_data):\n    data = {}\n    for quali in json_data[\"MRData\"][\"RaceTable\"][\"Races\"]:\n        for result in quali[\"QualifyingResults\"]:\n            data.setdefault(\"circuit\", []).append(quali[\"Circuit\"][\"circuitId\"])\n            data.setdefault(\"driver\", []).append(result[\"Driver\"][\"code\"])\n            data.setdefault(\"round\", []).append(quali[\"round\"])\n            data.setdefault(\"constructor\", []).append(\n                result[\"Constructor\"][\"constructorId\"]\n            )\n\n            data.setdefault(\"qualifying_position\", []).append(result[\"position\"])\n\n    return (\n        pd.DataFrame(data)\n        .astype(\n            {\n                \"circuit\": \"category\",\n                \"driver\": \"category\",\n                \"round\": \"int\",\n                \"constructor\": \"category\",\n                \"qualifying_position\": \"Int64\",\n            }\n        )\n        .set_index([\"circuit\", \"driver\"])\n    )\n\n\n\ngp_df = gp_results_to_dataframe(gp_data)\ngp_df\n\n\n\n\n\n\n\n\n\nround\nconstructor\ngrid\nposition\npoints\nfastest_lap\nstatus\n\n\ncircuit\ndriver\n\n\n\n\n\n\n\n\n\n\n\nbahrain\nVER\n1\nred_bull\n1\n1\n25.0\n6\nFinished\n\n\nPER\n1\nred_bull\n2\n2\n18.0\n7\nFinished\n\n\nALO\n1\naston_martin\n5\n3\n15.0\n5\nFinished\n\n\nSAI\n1\nferrari\n4\n4\n12.0\n14\nFinished\n\n\nHAM\n1\nmercedes\n7\n5\n10.0\n10\nFinished\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nzandvoort\nMAG\n13\nhaas\n0\n16\n0.0\n17\nFinished\n\n\nRUS\n13\nmercedes\n3\n17\n0.0\n14\nFinished\n\n\nZHO\n13\nalfa\n5\n18\n0.0\n16\nAccident\n\n\nLEC\n13\nferrari\n9\n19\n0.0\n19\nUndertray\n\n\nSAR\n13\nwilliams\n10\n20\n0.0\n20\nAccident\n\n\n\n\n260 rows × 7 columns\n\n\n\n\nquali_df = quali_results_to_dataframe(quali_data)\nquali_df\n\n\n\n\n\n\n\n\n\nround\nconstructor\nqualifying_position\n\n\ncircuit\ndriver\n\n\n\n\n\n\n\nbahrain\nVER\n1\nred_bull\n1\n\n\nPER\n1\nred_bull\n2\n\n\nLEC\n1\nferrari\n3\n\n\nSAI\n1\nferrari\n4\n\n\nALO\n1\naston_martin\n5\n\n\n...\n...\n...\n...\n...\n\n\nzandvoort\nZHO\n13\nalfa\n16\n\n\nOCO\n13\nalpine\n17\n\n\nMAG\n13\nhaas\n18\n\n\nBOT\n13\nalfa\n19\n\n\nLAW\n13\nalphatauri\n20\n\n\n\n\n260 rows × 3 columns\n\n\n\nFinally, we join the grand prix and qualifying dataframes together, forming the base dataset for our analysis; we also rename all entries credited to Liam Lawson (LAW) to Daniel Ricciardio (RIC), as Fantasy GP credits points from the temporary reserve driver to the driver they replace:\n\ndata_df = gp_df.join(quali_df[[\"qualifying_position\"]]).rename(\n    level=\"driver\", index={\"LAW\": \"RIC\"}\n)\ndata_df\n\n\n\n\n\n\n\n\n\nround\nconstructor\ngrid\nposition\npoints\nfastest_lap\nstatus\nqualifying_position\n\n\ncircuit\ndriver\n\n\n\n\n\n\n\n\n\n\n\n\nbahrain\nVER\n1\nred_bull\n1\n1\n25.0\n6\nFinished\n1\n\n\nPER\n1\nred_bull\n2\n2\n18.0\n7\nFinished\n2\n\n\nALO\n1\naston_martin\n5\n3\n15.0\n5\nFinished\n5\n\n\nSAI\n1\nferrari\n4\n4\n12.0\n14\nFinished\n4\n\n\nHAM\n1\nmercedes\n7\n5\n10.0\n10\nFinished\n7\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nzandvoort\nMAG\n13\nhaas\n0\n16\n0.0\n17\nFinished\n18\n\n\nRUS\n13\nmercedes\n3\n17\n0.0\n14\nFinished\n3\n\n\nZHO\n13\nalfa\n5\n18\n0.0\n16\nAccident\n16\n\n\nLEC\n13\nferrari\n9\n19\n0.0\n19\nUndertray\n9\n\n\nSAR\n13\nwilliams\n10\n20\n0.0\n20\nAccident\n10\n\n\n\n\n260 rows × 8 columns"
  },
  {
    "objectID": "posts/fantasy-gp-mcmc/fantasy-gp-mcmc.html#fantasy-points",
    "href": "posts/fantasy-gp-mcmc/fantasy-gp-mcmc.html#fantasy-points",
    "title": "Modeling Formula 1 Fantasy GP points with PyMC",
    "section": "2 Fantasy points",
    "text": "2 Fantasy points\nFantasy GP points per team/driver are calculated in the following way: * drivers earn the points scored during the grand prix, 10pts for taking pole position, 2pts per position gained on Sunday, and 5 points each for beating their teammate during qualifying and the race itself. * constructors earn the points scored by each of their drivers during the grand prix, 1pt per position gained per driver, 5 points if both cars finish, 2 if only one car finishes.\nTo simplify the problem, we completely ignore sprint races (as should everyone).\nThe race finish status classification has several detailed categories:\n\ndata_df[\"status\"].dtype\n\nCategoricalDtype(categories=['+1 Lap', '+2 Laps', 'Accident', 'Brakes', 'Collision',\n                  'Collision damage', 'Electrical', 'Engine', 'Finished',\n                  'Mechanical', 'Oil leak', 'Overheating', 'Power loss',\n                  'Retired', 'Undertray'],\n, ordered=False, categories_dtype=object)\n\n\nFantasy GP scoring only cares whether a car finished or not, so we define the following set of categories to indiciate a finish:\n\nSTATUS_FINISHED = [\"Finished\", \"+1 Lap\", \"+2 Laps\"]\n\nWhile our dataset has the WDC points scored per driver/constructor already, positions gained and performance vs. teammate need to be added in:\n\ndef add_scoring_cols(df: pd.DataFrame) -&gt; pd.DataFrame:\n    return df.assign(\n        # Grid value of 0 indicates pit lane start; here we set that to 99\n        # to simplify the check for who won out in qualifying.\n        grid=lambda x: x[\"grid\"].where(x[\"grid\"] != 0, 20)\n    ).assign(\n        # Positions gained compared to the starting grid position; scoring doesn't\n        # care about positions lost, so we set anything below 0 to 0.\n        positions_gained=lambda x: np.maximum(x[\"grid\"] - x[\"position\"], 0),\n        # Whether the driver won pole position\n        has_pole=lambda x: x[\"qualifying_position\"] == 1,\n        # Whether the driver beat their teammate in qualifying\n        beat_teammate_quali=lambda x: x.groupby(\n            # Each group is per race, per constructor, so only 2 rows - one for each driver.\n            [\"circuit\", \"constructor\"],\n            group_keys=False,\n        ).apply(\n            # Smaller grid pos. = better; the grid position is compared\n            # to the reversed grid array in the group (essentially\n            # we create a cartesian product of the grid pos.)\n            lambda g: g[\"grid\"]\n            &lt; g[\"grid\"].iloc[::-1].values\n        ),\n        # Same as the previous column, but for finishing position in the race.\n        beat_teammate_race=lambda x: x.groupby(\n            [\"circuit\", \"constructor\"], group_keys=False\n        ).apply(\n            lambda g: (g[\"position\"] &lt; g[\"position\"].iloc[::-1].values)\n            & g[\"status\"].isin(STATUS_FINISHED)\n        ),\n        has_fastest_lap=lambda x: x[\"fastest_lap\"] == 1,\n    )\n\n\ndata_df = add_scoring_cols(data_df)\n\nThis leaves us with the following dataframe:\n\n\nCode\ndata_df[\n    [\n        \"grid\",\n        \"positions_gained\",\n        \"beat_teammate_quali\",\n        \"beat_teammate_race\",\n        \"has_pole\",\n        \"has_fastest_lap\",\n    ]\n]\n\n\n\n\n\n\n\n\n\n\ngrid\npositions_gained\nbeat_teammate_quali\nbeat_teammate_race\nhas_pole\nhas_fastest_lap\n\n\ncircuit\ndriver\n\n\n\n\n\n\n\n\n\n\nbahrain\nVER\n1\n0\nTrue\nTrue\nTrue\nFalse\n\n\nPER\n2\n0\nFalse\nFalse\nFalse\nFalse\n\n\nALO\n5\n2\nTrue\nTrue\nFalse\nFalse\n\n\nSAI\n4\n0\nFalse\nTrue\nFalse\nFalse\n\n\nHAM\n7\n2\nFalse\nTrue\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nzandvoort\nMAG\n20\n4\nFalse\nFalse\nFalse\nFalse\n\n\nRUS\n3\n0\nTrue\nFalse\nFalse\nFalse\n\n\nZHO\n5\n0\nTrue\nFalse\nFalse\nFalse\n\n\nLEC\n9\n0\nFalse\nFalse\nFalse\nFalse\n\n\nSAR\n10\n0\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n260 rows × 6 columns\n\n\n\nFinally, we can score the Fantasy GP points; we define the scoring function for the driver to be the sum of: - WDC points - 10 for gaining pole position - 5 for outqualifying their teammate - 5 for outracing their teammate - 2 per position gained at the end of the race\nNote that a driver who did not finish the race on Sunday can still earn points from the qualifying session.\n\ndef score_driver(x):\n    return pd.Series(\n        x[\"has_pole\"] * 10 + x[\"beat_teammate_quali\"] * 5, dtype=float\n    ) + pd.Series(\n        x[\"points\"] + x[\"positions_gained\"] * 2 + x[\"beat_teammate_race\"] * 5,\n        dtype=float,\n    ).where(\n        x[\"status\"].isin(STATUS_FINISHED), 0\n    )\n\nScoring for the constructors is defined as: - WDC points - 5 points if both cars finished, 2 if only one did - 1 point per position gained at the end of the race per car\n\ndef score_constructor(x):\n    finished = x[\"status\"].isin(STATUS_FINISHED)\n    match finished.sum():\n        case 2:\n            finish_bonus = 5\n        case 1:\n            finish_bonus = 2\n        case _:\n            finish_bonus = 0\n\n    return x[\"points\"].sum() + x[finished][\"positions_gained\"].sum() + finish_bonus\n\nJust from the rules definition it is clear that drivers have a higher points earnign ceiling but much higher variance, whereas constructors are more consistent but earn fewer points.\n\n\nCode\ndef score_fantasy_points(df: pd.DataFrame) -&gt; pd.DataFrame:\n    return df.assign(\n        driver_points=score_driver,\n        # Constructor points need to be joined back in on the grouping columns, in order to\n        # fill in the missing spots with duplicate values - since we have 20 drivers, but 10 constructors,\n        # the group has fewer rows and needs to be broadcast per group on the index.\n        constructor_points=lambda x: x.join(\n            x.groupby([\"circuit\", \"constructor\"])\n            .apply(score_constructor)\n            .rename(\"constructor_points\"),\n            on=[\"circuit\", \"constructor\"],\n        )[\"constructor_points\"],\n    )\n\n\nThe scored dataframe looks like this:\n\n\nCode\ndata_df = score_fantasy_points(data_df)\ndata_df[[\"driver_points\", \"constructor_points\"]]\n\n\n\n\n\n\n\n\n\n\ndriver_points\nconstructor_points\n\n\ncircuit\ndriver\n\n\n\n\n\n\nbahrain\nVER\n45.0\n48.0\n\n\nPER\n18.0\n48.0\n\n\nALO\n29.0\n32.0\n\n\nSAI\n17.0\n14.0\n\n\nHAM\n19.0\n23.0\n\n\n...\n...\n...\n...\n\n\nzandvoort\nMAG\n8.0\n11.0\n\n\nRUS\n5.0\n20.0\n\n\nZHO\n5.0\n6.0\n\n\nLEC\n0.0\n13.0\n\n\nSAR\n0.0\n6.0\n\n\n\n\n260 rows × 2 columns\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe constructor points are duplicated per race, since we broadcast those across both drivers."
  },
  {
    "objectID": "posts/fantasy-gp-mcmc/fantasy-gp-mcmc.html#data-exploration",
    "href": "posts/fantasy-gp-mcmc/fantasy-gp-mcmc.html#data-exploration",
    "title": "Modeling Formula 1 Fantasy GP points with PyMC",
    "section": "3 Data exploration",
    "text": "3 Data exploration\nNow we visualize the data to get a better idea of its properties. First, some basic box plots for each driver/constructor:\n\n\nCode\nsns.set_theme(style=\"ticks\")\nf, ax = plt.subplots(figsize=(12, 10))\nsns.boxplot(\n    data=data_df.reset_index(),\n    x=\"driver_points\",\n    y=\"driver\",\n    palette=\"pastel\",\n    whis=[0, 100],\n)\n\nsns.stripplot(\n    x=\"driver_points\",\n    y=\"driver\",\n    data=data_df.reset_index(),\n    hue=\"circuit\",\n    linewidth=0,\n    size=5,\n    dodge=True,\n    palette=\"muted\",\n)\n\nax.xaxis.grid(True)\nax.set(xlabel=\"points\")\nax.set_title(\"Driver Fantasy GP points 2023\");\n\n\n\n\n\nFor convenience, we create a constructor-specific dataframe by taking the first row for each constructor for each race. This gets rid of the duplicate data (as constructor points were duplicated across both drivers):\n\nconst_data_df = data_df.groupby([\"circuit\", \"constructor\"]).first().reset_index()\n\n\n\nCode\nsns.set_theme(style=\"ticks\")\nf, ax = plt.subplots(figsize=(12, 6))\nsns.boxplot(\n    data=const_data_df.reset_index(),\n    x=\"constructor_points\",\n    y=\"constructor\",\n    palette=\"pastel\",\n    whis=[0, 100],\n)\n\nsns.stripplot(\n    x=\"constructor_points\",\n    y=\"constructor\",\n    data=const_data_df.reset_index(),\n    hue=\"circuit\",\n    palette=\"muted\",\n    linewidth=0,\n    size=4,\n    dodge=True,\n)\n\nax.xaxis.grid(True)\nax.set(xlabel=\"points\")\nax.set_title(\"Constructor Fantasy GP points 2023\");\n\n\n\n\n\nThese look quite similar and expose the brutal reality of Red Bull’s total dominance over the field. Matching our intuition from the rules definition, drivers appear to earn more and have wider variance in points scored, comapred to teams, with the exception of Max Verstappen and his absurd consistency.\nNext, we plot the kernel density estimate of the driver points to gain a better idea of the overall distribution:\n\n\nCode\ng = sns.displot(\n    data_df.reset_index(),\n    x=\"driver_points\",\n    col=\"driver\",\n    kind=\"kde\",\n    rug=True,\n    col_wrap=3,\n    fill=True,\n)\ng.set_axis_labels(\"points\", \"density\");\n\n\n\n\n\n\n\nCode\ng = sns.displot(\n    const_data_df.reset_index(),\n    x=\"constructor_points\",\n    col=\"constructor\",\n    col_wrap=5,\n    kind=\"kde\",\n    height=4,\n    rug=True,\n    fill=True,\n)\n\n\ng.set_axis_labels(\"points\", \"density\");\n\n\n\n\n\nThe distribution of points is spread and somewhat multi-peaked in both cases; again, constructors appear score in a much more consistent range compared to drivers. The pooled observed points distributions are:\n\n\nCode\nf, axs = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\nsns.kdeplot(data_df.reset_index(), x=\"driver_points\", fill=True, ax=axs[0])\nsns.kdeplot(const_data_df.reset_index(), x=\"constructor_points\", fill=True, ax=axs[1])\naxs[0].set_title(\"Overall driver points density 2023\")\naxs[0].set(xlabel=\"driver points\")\n\naxs[1].set_title(\"Overall constructor points density 2023\")\naxs[1].set(xlabel=\"constructor points\")\n\nf.tight_layout();\n\n\n\n\n\nGiven that the point distributions between drivers and constructors appear very similar, we’ll focus on constructing models of the driver points, which we can later fit to the constructor data with minor adjustments."
  },
  {
    "objectID": "posts/fantasy-gp-mcmc/fantasy-gp-mcmc.html#unpooled-model",
    "href": "posts/fantasy-gp-mcmc/fantasy-gp-mcmc.html#unpooled-model",
    "title": "Modeling Formula 1 Fantasy GP points with PyMC",
    "section": "4 Unpooled model",
    "text": "4 Unpooled model\nOur first model will be the simplest possible - we just directly estimate the mean points for each driver individually (an intercept-only, unpooled linear model).\n\n4.1 Picking the likelihood\nThe points data has several properties that stem from the Fantasy GP ruleset: - points are discrete - points are strictly bound between 0 and 84 (25 for p1 + 1 for fastest lap + 10 for pole position + 10 for beating teammate + 19 * 2 for maximum possible positions gained) - most drivers have a few 0 point finishes, but overall it’s very unlikely for a driver to score 0 points consistently.\nBecause we are modeling discrete, strictly positive points, one of the count distributions is likely an appropriate choice for the likelihood. The Poisson distribution could be a good choice, but the observations appear overdispersed, as measured by the coefficient of dispersion:\n\n\nCode\ndata_df = data_df.reset_index().astype(\n    {\n        \"circuit\": \"category\",\n        \"driver\": \"category\",\n    }\n)\n\ndispersion = (\n    data_df.groupby(\"driver\")[\"driver_points\"].var()\n    / data_df.groupby(\"driver\")[\"driver_points\"].mean()\n)\ndispersion\n\n\ndriver\nALB     2.158586\nALO     2.270430\nBOT     4.930283\nDEV     2.808279\nGAS    13.610345\nHAM     1.934783\nHUL     3.598765\nLEC     8.770227\nMAG     4.777778\nNOR     6.144891\nOCO     7.162602\nPER     3.986537\nPIA     6.518519\nRIC     2.818182\nRUS     6.504785\nSAI     4.422156\nSAR     7.111111\nSTR     5.604167\nTSU     2.145695\nVER     1.028269\nZHO     7.261111\nName: driver_points, dtype: float64\n\n\nGiven that, we will pick the Negative Binomial as our likelihood, since it handles overdispersion better1.\n\n\n4.2 Choosing the priors\nA Negative Binomial likelihood is defined by two parameters - its mean μ, and a shape parameter α. We’ll need to set priors on both.\nThe mean is restricted to be positive only. The standard way of defining a negative binomial linear model is to set a Normal prior on the mean and use the exponentiation as the inverse link function to make it positive and usable for parameterizing the likelihood. For the actual prior parameters it would be reasonable to go with a fairly uninformative prior, such as Normal(mu=0, sigma=1); in this case, prior knowledge and the ruleset gives us a hunch that we can reasonably expect most drivers’ mean points per race to be somewhere between 5 (backmarker, occasionally beats their teammate, maybe gains a place) and 30 (top driver & team, consistently getting and winning from pole position).\nTo come up with parameter values for this, we use the PreliZ library to find a maximum entropy Normal distribution with 90% of its mass between 5 and 30 (we take the natural log of those, given the exponential inverse link):\n\nmu_dist = pz.Normal()\npz.maxent(mu_dist, np.log(5), np.log(30), 0.90);\n\n\n\n\nThe shape parameter α is also strictly positive. Its effect on the likelihood can be best understood by plotting a Negative Binomial for a few different values, while keeping the mean fixed:\n\n\nCode\nf, axs = plt.subplots(2, 2, figsize=(12, 6), sharey=True)\n\npz.NegativeBinomial(alpha=0.5, mu=15).plot_pdf(ax=axs[0, 0], legend=False)\naxs[0, 0].set_title(f\"Negative Binomial with μ: 15, α: 0.5\")\n\npz.NegativeBinomial(alpha=5, mu=15).plot_pdf(ax=axs[0, 1], legend=False)\naxs[0, 1].set_title(f\"Negative Binomial with μ: 15, α: 5\")\n\npz.NegativeBinomial(alpha=50, mu=15).plot_pdf(ax=axs[1, 0], legend=False)\naxs[1, 0].set_title(f\"Negative Binomial with μ: 15, α: 50\")\n\npz.NegativeBinomial(alpha=500, mu=15).plot_pdf(ax=axs[1, 1], legend=False)\naxs[1, 1].set_title(f\"Negative Binomial with μ: 15, α: 500\");\n\n\n\n\n\nAs α increases, the tails shrink. We don’t want very high values, as that will restrict the tails too much, but very small values result in absurdly long tails. To handle that, we use a Gamma distribution, with mass strongly concentrated between 1 and 20:\n\nalpha_dist = pz.Gamma()\npz.maxent(alpha_dist, 1, 20, .9);\n\n\n\n\nFinally, we prepare the data for use within the model. We factorize the driver column into labels and an index:\n\nidx, labels = pd.factorize(data_df[\"driver\"], sort=True)\n\nlabels is a categorical variable over the driver names:\n\n\nCategoricalIndex(['ALB', 'ALO', 'BOT', 'DEV', 'GAS', 'HAM', 'HUL', 'LEC',\n                  'MAG', 'NOR', 'OCO', 'PER', 'PIA', 'RIC', 'RUS', 'SAI',\n                  'SAR', 'STR', 'TSU', 'VER', 'ZHO'],\n                 categories=['ALB', 'ALO', 'BOT', 'DEV', ..., 'STR', 'TSU', 'VER', 'ZHO'], ordered=False, dtype='category')\n\n\nidx is the flattened (race number, driver) index into the observed race result rows:\n\n\narray([19, 11,  1, 15,  5, 17, 14,  2,  4,  0, 18, 16,  8,  3,  6, 20,  9,\n       10,  7, 12, 11, 19,  1, 14,  5, 15,  7, 10,  4,  8, 18,  6, 20,  3,\n       12, 16,  9,  2,  0, 17, 19,  5,  1, 17, 11,  9,  6, 12, 20, 18,  2,\n       15,  4, 10,  3, 16,  8, 14,  0,  7, 11, 19,  7,  1, 15,  5, 17, 14,\n        9, 18, 12,  0,  8,  4, 10, 16,  6,  2, 20,  3, 19, 11,  1, 14, 15,\n        5,  7,  4, 10,  8, 18, 17,  2,  0,  6, 20,  9,  3, 12, 16, 19,  1,\n       10,  5, 14,  7,  4, 15,  9, 12,  2,  3, 20,  0, 18, 11,  6, 16,  8,\n       17, 19,  5, 14, 11, 15, 17,  1, 10, 20,  4,  7, 18, 12,  3,  6,  0,\n        9,  8,  2, 16, 19,  1,  5,  7, 15, 11,  0, 10, 17,  2, 12,  4,  9,\n       18,  6, 20,  8,  3, 14, 16, 19,  7, 11,  9,  1, 15, 14,  5, 17,  4,\n        0, 20, 16, 10,  2, 12,  3,  8, 18,  6, 19,  9,  5, 12, 14, 11,  1,\n        0,  7, 15, 16,  2,  6, 17, 20, 18,  3,  4,  8, 10, 19,  9, 11,  5,\n       12, 14,  7, 15,  1, 17,  0,  2, 13,  6, 18, 20,  8, 16, 10,  4, 19,\n       11,  7,  5,  1, 14,  9, 10, 17, 18,  4,  2, 20,  0,  8, 13, 16,  6,\n       15, 12, 19,  1,  4, 11, 15,  5,  9,  0, 12, 10, 17,  6, 13,  2, 18,\n        8, 14, 20,  7, 16])\n\n\nWe also extract the observed points:\n\ndriver_points = data_df[\"driver_points\"]\ndriver_points\n\n0      45.0\n1      18.0\n2      29.0\n3      17.0\n4      19.0\n       ... \n255     8.0\n256     5.0\n257     5.0\n258     0.0\n259     0.0\nName: driver_points, Length: 260, dtype: float64\n\n\n\n\n4.3 Model definition\nNext, we define the model with PyMC. As discussed previously, we think that the process that generates points can be seen as a Negative Binomial distribution, so that is set as the likelihood. The mean (μ) аnd shape (α) are the parameters that we want to infer from the data. Finally, the prior guesses for these parameters are set from the mu_dist and alpha_dist objects we computed previously.\nThis translates to:\n\nwith pm.Model(coords={\"driver\": labels, \"idx\": idx}) as unpooled_negb:\n    μ = pm.Normal(\"μ\", mu_dist.mu, mu_dist.sigma, dims=\"driver\")\n    μ_ = pm.Deterministic(\"μ_\", pm.math.exp(μ), dims=\"driver\")\n\n    α = pm.Gamma(\"α\", alpha_dist.alpha, alpha_dist.beta, dims=\"driver\")\n\n    y = pm.NegativeBinomial(\n        \"y\", alpha=α[idx], mu=μ_[idx], observed=driver_points, dims=\"idx\"\n    )\n\n\n\n4.4 Prior predictive checks\nTo see if the choice for priors is acceptable, we sample from the prior predictive and plot the results:\n\nwith unpooled_negb:\n    unpooled_prior_samples = pm.sample_prior_predictive(samples=1000)\n\nSampling: [y, α, μ]\n\n\n\naz.plot_ppc(unpooled_prior_samples, group=\"prior\", figsize=(12, 6));\n\n\n\n\nThe prior predictive is quite wide, with a very long and somewhat fat tail, but overall the data could be plausibly seen as coming from it.\n\n\n4.5 Inference and diagnostics\nNext, we perform the actual inference step - we sample from the posterior and the posterior predictive distribution:\n\nwith unpooled_negb:\n    unpooled_idata = pm.sample(target_accept=0.9, idata_kwargs={\"log_likelihood\": True})\n    pm.sample_posterior_predictive(unpooled_idata, extend_inferencedata=True)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [μ, α]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 5 seconds.\nSampling: [y]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:04&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\nThere are no divergences detected, which is a good first sign that the model has converged.\nTo check the sampling process we plot the traces for the μ parameter. What we are looking for is similar densities on the left (indicating that the chains have converged to the same posterior distribution), and noisy/patternless trace plots on the right (which would indicate that the sampler was able to efficiently run through the sample space)2:\n\naz.plot_trace(unpooled_idata, var_names=[\"μ\"], compact=False);\n\n\n\n\nThere are a few kinks here and there (see RIC), but overall these look good.\nNext we plot the energy plot and BFMI. Ideally the marginal energy and energy transition distributions should overlap as much as possible, and BFMI should be to be above .3:\n\n\nCode\nax = az.plot_energy(unpooled_idata)\nax.set_title(\"Energy Plot\");\n\n\n\n\n\nFinally, we check the summary statistics for the μ parameter: - bulk and tail effective sample size (ess_bulk and ess_tail respectively) should be &gt; 400, otherwise the estimation of the rest of the summary statistics is considered unreliable 3 - r_hat should ideally equal 1, with values over 1.01 convergence issues 4 - mcse_{mean, sd} is the Monte Carlo Standard Error, or the error introduced by approximating the posterior with a finite number of samples; lower = better 5\n\naz.summary(unpooled_idata, var_names=[\"μ\"])\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nμ[ALB]\n2.536\n0.117\n2.330\n2.761\n0.002\n0.001\n4320.0\n2693.0\n1.0\n\n\nμ[ALO]\n3.153\n0.102\n2.959\n3.339\n0.002\n0.001\n4155.0\n2788.0\n1.0\n\n\nμ[BOT]\n2.476\n0.195\n2.127\n2.876\n0.003\n0.002\n3924.0\n2310.0\n1.0\n\n\nμ[DEV]\n1.761\n0.214\n1.347\n2.150\n0.003\n0.002\n4724.0\n2339.0\n1.0\n\n\nμ[GAS]\n2.448\n0.243\n2.000\n2.917\n0.004\n0.003\n4174.0\n2716.0\n1.0\n\n\nμ[HAM]\n3.119\n0.097\n2.936\n3.303\n0.002\n0.001\n4028.0\n2484.0\n1.0\n\n\nμ[HUL]\n2.156\n0.163\n1.842\n2.461\n0.003\n0.002\n3949.0\n2420.0\n1.0\n\n\nμ[LEC]\n2.732\n0.216\n2.339\n3.152\n0.003\n0.002\n4608.0\n3103.0\n1.0\n\n\nμ[MAG]\n2.082\n0.272\n1.568\n2.578\n0.005\n0.004\n3401.0\n2402.0\n1.0\n\n\nμ[NOR]\n2.716\n0.155\n2.430\n3.027\n0.002\n0.002\n4446.0\n2606.0\n1.0\n\n\nμ[OCO]\n2.329\n0.254\n1.873\n2.814\n0.004\n0.003\n3931.0\n2496.0\n1.0\n\n\nμ[PER]\n3.288\n0.108\n3.090\n3.497\n0.002\n0.001\n4118.0\n2353.0\n1.0\n\n\nμ[PIA]\n1.940\n0.296\n1.358\n2.476\n0.005\n0.004\n4082.0\n2374.0\n1.0\n\n\nμ[RIC]\n2.421\n0.259\n1.960\n2.920\n0.004\n0.003\n3907.0\n2528.0\n1.0\n\n\nμ[RUS]\n2.753\n0.174\n2.437\n3.092\n0.003\n0.002\n4625.0\n2669.0\n1.0\n\n\nμ[SAI]\n2.554\n0.173\n2.228\n2.865\n0.003\n0.002\n4475.0\n2856.0\n1.0\n\n\nμ[SAR]\n1.917\n0.472\n1.040\n2.794\n0.007\n0.005\n4408.0\n3437.0\n1.0\n\n\nμ[STR]\n2.203\n0.280\n1.707\n2.748\n0.005\n0.004\n3535.0\n2543.0\n1.0\n\n\nμ[TSU]\n2.455\n0.120\n2.220\n2.671\n0.002\n0.001\n4800.0\n3020.0\n1.0\n\n\nμ[VER]\n3.747\n0.076\n3.599\n3.892\n0.001\n0.001\n4629.0\n2855.0\n1.0\n\n\nμ[ZHO]\n2.290\n0.239\n1.839\n2.736\n0.004\n0.003\n4797.0\n2443.0\n1.0\n\n\n\n\n\n\n\nAll of the sampling diagnistics look good, which means the model has converged and we can move on to posterior predictive checks and model evaluation.\n\n\n4.6 Posterior predictive checks\nNext we examine the posterior predictive distribution, which will help validate that the predictions the model is making make sense given the data. Below we plot several posterior graphs (from top to bottom, left to right): - the (grouped) posterior predictive distribution should closely replicate the pattern seen in the observed data 6 - the Bayesian p-value is the probability of the predicted data being equal to or less than the observed data. The ideal/expected outcome is represented by the dashed line (0.5), with the solid line being the KDE of the predicted data; the u-value represents the same idea as the p-value, but per observation - it should be roughly uniform 7 - LOO-PIT is the probability integral transform checed with LOO-CV; this should be close to uniform; difference between LOO-PIT empirical cumulative distribution function and the uniform cdf 8\n\n\nCode\ndef plot_posterior_predictive_checks(idata):\n    f = plt.figure(figsize=(12, 12))\n\n    ax_ppc = f.add_subplot(3, 1, 1)\n    az.plot_ppc(idata, ax=ax_ppc)\n    ax_ppc.set_title(\"Posterior predictive checks\")\n\n    ax_bpv = f.add_subplot(3, 2, 3)\n    az.plot_bpv(idata, kind=\"p_value\", ax=ax_bpv)\n    ax_bpv.set_title(\"Bayesian p_value\")\n\n    ax_uv = f.add_subplot(3, 2, 4)\n    az.plot_bpv(idata, kind=\"u_value\", ax=ax_uv)\n    ax_uv.set_title(\"Marginal p_value\")\n\n    ax_loo_pit_ecdf = f.add_subplot(3, 2, 5)\n    az.plot_loo_pit(idata, y=\"y\", ecdf=True, ax=ax_loo_pit_ecdf)\n    ax_loo_pit_ecdf.set_title(\"LOO-PIT (ecdf=True)\")\n\n    ax_loo_pit = f.add_subplot(3, 2, 6)\n    az.plot_loo_pit(idata, y=\"y\", ecdf=False, ax=ax_loo_pit)\n    ax_loo_pit.set_title(\"LOO-PIT (ecdf=False)\")\n\n    f.tight_layout()\n\n    return (ax_ppc, ax_bpv, ax_uv, ax_loo_pit_ecdf, ax_loo_pit)\n\n\n\nplot_posterior_predictive_checks(unpooled_idata);\n\n\n\n\nFrom the posterior predictive plots it seems that: - the posterior predictive samples roughly look like tha observed data, but the fit around the right tail isn’t the best. - the p-value plot shows that the model’s predictions are somewhat shifted to the right and the u-value plot indicates that the model is missing observations in the left tail 9 - LOO-PIT shows the model is likely biased, which agrees with the p/u-value plots.\nLastly, let’s plot the posterior predictive distributions for a few individual drivers:\n\n\nCode\nfig, axs = plt.subplots(2, 2, figsize=(12, 6))\n\nDriverIdx = Enum(\"Driver\", zip(labels.categories, labels.codes))\n\naz.plot_ppc(unpooled_idata, coords={\"idx\": [DriverIdx.VER.value]}, ax=axs[0, 0])\naxs[0, 0].set_title(\"VER\")\n\naz.plot_ppc(unpooled_idata, coords={\"idx\": [DriverIdx.SAR.value]}, ax=axs[0, 1])\naxs[0, 1].set_title(\"SAR\")\n\naz.plot_ppc(unpooled_idata, coords={\"idx\": [DriverIdx.LEC.value]}, ax=axs[1, 0])\naxs[1, 0].set_title(\"LEC\")\n\naz.plot_ppc(unpooled_idata, coords={\"idx\": [DriverIdx.HAM.value]}, ax=axs[1, 1])\naxs[1, 1].set_title(\"HAM\");\n\n\n\n\n\n\n\n4.7 Posterior evaluation\nWe are reasonably confident that the model has converged and that there were no issues during sampling. Predictions seem to be somewhat biased, but still reasonable for our purposes. Now it’s time to check the posterior distributions of the estimated parameter μ:\n\naz.plot_posterior(unpooled_idata, var_names=[\"μ\"]);\n\n\n\n\nThe estimates look well formed; to make comparing the parameter value between drivers easier, we plot a forest of the 94% HDI 10:\n\naz.plot_forest(unpooled_idata, var_names=[\"μ\"], combined=True, figsize=(12, 6));\n\n\n\n\nThe probability distributions for the means broadly agree with our informal observations throughout the season - VER is scoring high above the rest, in an extremely consistent range, with HAM, ALO, and PER close to eachother for second place, a mixed, variable set of results for the midfield, and a few clear backmarker drivers.\nTo translate the estimated parameter values to points, we can plot the transformed μ_ value (remember that this is just the exponentiated μ):\n\naz.plot_forest(unpooled_idata, var_names=[\"μ_\"], combined=True, figsize=(12, 6));\n\n\n\n\n\n\n4.8 Model evaluation\nFinally, we inspect PSIS-LOO-CV to get an idea of the performance of the model:\n\naz.loo(unpooled_idata)\n\nComputed from 4000 posterior samples and 260 observations log-likelihood matrix.\n\n         Estimate       SE\nelpd_loo  -889.76    14.58\np_loo       43.98        -\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.5]   (good)      254   97.7%\n (0.5, 0.7]   (ok)          6    2.3%\n   (0.7, 1]   (bad)         0    0.0%\n   (1, Inf)   (very bad)    0    0.0%\n\n\nelpd_loo 11 will be more useful when comparing this baseline model to other, more complicated ones, but the fact that Pareto k values are all significantly below 0.7 is a good sign that the model can generalize decently."
  },
  {
    "objectID": "posts/fantasy-gp-mcmc/fantasy-gp-mcmc.html#zero-inflated-unpooled-model",
    "href": "posts/fantasy-gp-mcmc/fantasy-gp-mcmc.html#zero-inflated-unpooled-model",
    "title": "Modeling Formula 1 Fantasy GP points with PyMC",
    "section": "5 Zero-inflated unpooled model",
    "text": "5 Zero-inflated unpooled model\nOne of the common issues with both Poisson and Negative Binomial models is that they do not generate zeroes often enough. This can be alleviated by using a mixture model that inflates the number of zeroes, which PyMC already has an implementation of - the Zero Inflated Negative Binomial likelihood.\nTo test that extra zeroes might result in a better fit, we define a new model with the same priors as before, but using the mixture likelihood. One additional parameter, ψ, is needed, so we have to set a prior over that. ψ is the probability of a non-zero value, between 0 and 1. We will use a Beta as the prior distribution, and given the ruleset and reliability of modern F1 cars, our prior belief is that 0 point finishes aren’t all that likely. To express that, we take the maximum entropy Beta whose mass is concentrated between .8 and .99:\n\n5.1 Choosing priors\n\npsi_dist = pz.Beta()\npz.maxent(psi_dist, 0.8, 0.99, 0.9);\n\n\n\n\n\n\n5.2 Model definition\nThe new model is trivial to define:\n\nwith pm.Model(coords={\"driver\": labels, \"idx\": idx}) as zi_unpooled_negb:\n    μ = pm.Normal(\"μ\", mu_dist.mu, mu_dist.sigma, dims=\"driver\")\n    μ_ = pm.Deterministic(\"μ_\", pm.math.exp(μ), dims=\"driver\")\n\n    α = pm.Gamma(\"α\", alpha_dist.alpha, alpha_dist.beta, dims=\"driver\")\n\n    ψ = pm.Beta(\"ψ\", psi_dist.alpha, psi_dist.beta, dims=\"driver\")\n\n    y = pm.ZeroInflatedNegativeBinomial(\n        \"y\",\n        psi=ψ[idx],\n        alpha=α[idx],\n        mu=μ_[idx],\n        observed=driver_points,\n        dims=\"idx\",\n    )\n\n\n\n5.3 Prior evaluation\nWe perform the same prior predictive check:\n\nwith zi_unpooled_negb:\n    zi_unpooled_prior_samples = pm.sample_prior_predictive(samples=1000)\n\nSampling: [y, α, μ, ψ]\n\n\n\naz.plot_ppc(zi_unpooled_prior_samples, group=\"prior\");\n\n\n\n\n\n\n5.4 Inference and diagnostics\nSample:\n\nwith zi_unpooled_negb:\n    zi_unpooled_idata = pm.sample(\n        target_accept=0.9, idata_kwargs={\"log_likelihood\": True}\n    )\n    pm.sample_posterior_predictive(zi_unpooled_idata, extend_inferencedata=True)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [μ, α, ψ]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 10 seconds.\nSampling: [y]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:09&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:05&lt;00:00]\n    \n    \n\n\nThe traceplots look about as good as before:\n\naz.plot_trace(zi_unpooled_idata, var_names=[\"μ\"], compact=False);\n\n\n\n\nBFMI and energy are reasonable:\n\n\nCode\nax = az.plot_energy(zi_unpooled_idata)\nax.set_title(\"Energy Plot\");\n\n\n\n\n\nr_hat and ESS look good as well:\n\naz.summary(zi_unpooled_idata, var_names=[\"μ\"])\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nμ[ALB]\n2.541\n0.115\n2.325\n2.755\n0.001\n0.001\n6059.0\n2698.0\n1.00\n\n\nμ[ALO]\n3.153\n0.100\n2.952\n3.332\n0.001\n0.001\n6387.0\n2362.0\n1.00\n\n\nμ[BOT]\n2.626\n0.132\n2.364\n2.856\n0.002\n0.001\n6166.0\n2553.0\n1.00\n\n\nμ[DEV]\n1.905\n0.182\n1.574\n2.251\n0.002\n0.002\n6665.0\n3017.0\n1.00\n\n\nμ[GAS]\n2.487\n0.226\n2.071\n2.920\n0.003\n0.002\n6241.0\n2819.0\n1.00\n\n\nμ[HAM]\n3.118\n0.095\n2.927\n3.285\n0.001\n0.001\n5974.0\n2642.0\n1.00\n\n\nμ[HUL]\n2.214\n0.145\n1.928\n2.476\n0.002\n0.001\n6685.0\n2551.0\n1.00\n\n\nμ[LEC]\n2.900\n0.152\n2.615\n3.188\n0.002\n0.001\n5683.0\n2455.0\n1.00\n\n\nμ[MAG]\n2.314\n0.148\n2.040\n2.608\n0.002\n0.001\n6671.0\n2811.0\n1.00\n\n\nμ[NOR]\n2.718\n0.155\n2.430\n3.012\n0.002\n0.001\n7077.0\n3085.0\n1.00\n\n\nμ[OCO]\n2.507\n0.167\n2.194\n2.832\n0.002\n0.002\n4580.0\n2620.0\n1.00\n\n\nμ[PER]\n3.286\n0.108\n3.076\n3.481\n0.001\n0.001\n6277.0\n3305.0\n1.00\n\n\nμ[PIA]\n2.108\n0.209\n1.678\n2.471\n0.003\n0.002\n6063.0\n2942.0\n1.00\n\n\nμ[RIC]\n2.424\n0.251\n1.986\n2.922\n0.003\n0.002\n6185.0\n2844.0\n1.00\n\n\nμ[RUS]\n2.838\n0.149\n2.552\n3.110\n0.002\n0.001\n6603.0\n2818.0\n1.00\n\n\nμ[SAI]\n2.626\n0.145\n2.366\n2.909\n0.002\n0.001\n5862.0\n2395.0\n1.00\n\n\nμ[SAR]\n2.095\n0.250\n1.583\n2.540\n0.004\n0.003\n4158.0\n2271.0\n1.00\n\n\nμ[STR]\n2.450\n0.141\n2.191\n2.723\n0.002\n0.001\n6987.0\n2586.0\n1.00\n\n\nμ[TSU]\n2.457\n0.121\n2.233\n2.682\n0.002\n0.001\n6605.0\n2735.0\n1.01\n\n\nμ[VER]\n3.751\n0.076\n3.610\n3.896\n0.001\n0.001\n6206.0\n2875.0\n1.00\n\n\nμ[ZHO]\n2.484\n0.156\n2.193\n2.781\n0.002\n0.002\n5348.0\n2711.0\n1.00\n\n\n\n\n\n\n\n\n\n5.5 Posterior predictive checks\nPosterior predictive checks look somewhat better: - the posterior predictive fits about as well, with a slightly thinner tail - p-value is still shifted - u-value is much closer to uniform - LOO-PIT still suggests bias, but it looks slightly better overall; the right tail of the posterior predictive still looks too fat and goes well above the maximum points possible in the ruleset, likely causing the bias and shifted predictions.\n\nplot_posterior_predictive_checks(zi_unpooled_idata);\n\n\n\n\n\n\n5.6 Posterior evaluation\nWe plot just the forest of transformed means:\n\naz.plot_forest(zi_unpooled_idata, var_names=[\"μ_\"], combined=True, figsize=(12, 6));\n\n\n\n\nAs expected, compared to the basic model, the means have been pulled towards zero slightly and the HDIs have shrunk.\n\n\n5.7 Model comparison\nNow that we have two models it’s worth running a comparison to see which performs better 12:\n\n\nCode\ncomparison = az.compare(\n    {\"unpooled\": unpooled_idata, \"zero inflated unpooled\": zi_unpooled_idata}\n)\ncomparison\n\n\n\n\n\n\n\n\n\nrank\nelpd_loo\np_loo\nelpd_diff\nweight\nse\ndse\nwarning\nscale\n\n\n\n\nzero inflated unpooled\n0\n-857.195803\n37.556046\n0.000000\n0.860765\n15.440698\n0.000000\nFalse\nlog\n\n\nunpooled\n1\n-889.761483\n43.984838\n32.565679\n0.139235\n14.581579\n10.665155\nFalse\nlog\n\n\n\n\n\n\n\n\naz.plot_compare(comparison);\n\n\n\n\nThe zero-inflated model clearly seems to perform better 13."
  },
  {
    "objectID": "posts/fantasy-gp-mcmc/fantasy-gp-mcmc.html#driver-estimates",
    "href": "posts/fantasy-gp-mcmc/fantasy-gp-mcmc.html#driver-estimates",
    "title": "Modeling Formula 1 Fantasy GP points with PyMC",
    "section": "6 Driver estimates",
    "text": "6 Driver estimates\nWe have a reasonable looking, if a bit biased model for driver points, which we’ll now use to inform fantasy team construction.\nWhen picking drivers, we consider the following: - how many points per race is the driver expected to bring and how reliably - how likely is it that a given driver will outperform another driver, and by what magnitude - how underpriced/overpriced is a a driver given their performance\nWhile price analysis is out of the scope of this notebook, the points estimation model can help us with the first two questions.\nBelow, we compare each driver to their rivals, using the difference of predicted points per race. To do so, we take the predictions generated from the posterior predictive distribution for each driver, subtract the two vectors, and take the mean. This reduces the posterior distributions to a point estimate, but still incorporates the uncertainty of the model, as we work directly with values generated from the posterior. The value can be seen both as confidence driver x is better than driver y - higher is better, and as the points we expect driver x to earn over driver y on average.\n\n\n\n\n\n\nCaution\n\n\n\nThe plot below isn’t symmetric over the right diagonal - we are strictly comparing the driver on the X axis to the driver on the Y axis.\n\n\n\ndef make_comparison(idata, idx, groups, size=1000):\n    columns = {}\n    for idx_a, name_a in enumerate(groups):\n        for idx_b, name_b in enumerate(groups):\n            pp_a = idata.posterior_predictive[\"y\"][:, :, idx == idx_a].values.flatten()\n            pp_b = idata.posterior_predictive[\"y\"][:, :, idx == idx_b].values.flatten()\n\n            samples_a = np.random.choice(pp_a, size=size)\n            samples_b = np.random.choice(pp_b, size=size)\n\n            if idx_a != idx_b:\n                points_over = (samples_a - samples_b).mean()\n            else:\n                points_over = 0\n\n            columns.setdefault(name_a, []).append(points_over)\n\n    comparison_df = pd.DataFrame.from_dict(columns).set_index(groups)\n    mask = np.triu(np.ones_like(comparison_df, dtype=bool))\n\n    return comparison_df, mask\n\n\ncomparison_df, mask = make_comparison(zi_unpooled_idata, idx, labels)\n\n\n\nCode\nsns.set_theme(style=\"white\")\n\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\nf, ax = plt.subplots(figsize=(11, 9))\n\nax = sns.heatmap(\n    comparison_df,\n    cmap=cmap,\n    mask=mask,\n    annot=True,\n    center=0,\n    square=True,\n    linewidths=0.5,\n    cbar_kws={\"shrink\": 0.5},\n    annot_kws={\"fontsize\": 8},\n).set(title=\"Driver points expected over rival\");"
  },
  {
    "objectID": "posts/fantasy-gp-mcmc/fantasy-gp-mcmc.html#constructor-estimates",
    "href": "posts/fantasy-gp-mcmc/fantasy-gp-mcmc.html#constructor-estimates",
    "title": "Modeling Formula 1 Fantasy GP points with PyMC",
    "section": "7 Constructor estimates",
    "text": "7 Constructor estimates\nWe’ll run the same loop for the constructor comparison - first we define a slightly modified zero inflated model, then we sample, validate convergence, and finally use the posterior and posterior predictive to evaluate constructor peformance.\nData preparation is identical - we factorize the constructors and take the points\n\nconst_idx, const_labels = pd.factorize(const_data_df[\"constructor\"], sort=True)\nconst_points = const_data_df[\"constructor_points\"]\n\nWe adjust the priors slightly, to account for differences in the constructors scoring system:\n\nmu_const_dist = pz.Normal()\npz.maxent(mu_const_dist, np.log(2), np.log(30), 0.9);\n\n\n\n\n\nalpha_const_dist = pz.Gamma()\npz.maxent(alpha_const_dist, 1, 10, .9);\n\n\n\n\n\npsi_const_dist = pz.Beta()\npz.maxent(psi_const_dist, .9, .99, .9);\n\n\n\n\n\nwith pm.Model(\n    coords={\"constructor\": const_labels, \"idx\": const_idx}\n) as const_zi_unpooled_model:\n    μ = pm.Normal(\"μ\", mu_const_dist.mu, mu_const_dist.sigma, dims=\"constructor\")\n    μ_ = pm.Deterministic(\"μ_\", pm.math.exp(μ), dims=\"constructor\")\n\n    α = pm.Gamma(\"α\", alpha_const_dist.alpha, alpha_const_dist.beta, dims=\"constructor\")\n\n    ψ = pm.Beta(\"ψ\", psi_const_dist.alpha, psi_const_dist.beta, dims=\"constructor\")\n\n    y = pm.ZeroInflatedNegativeBinomial(\n        \"y\",\n        psi=ψ[const_idx],\n        alpha=α[const_idx],\n        mu=μ_[const_idx],\n        observed=const_points,\n        dims=\"idx\",\n    )\n\n\nwith const_zi_unpooled_model:\n    const_prior_pred = pm.sample_prior_predictive(samples=1000)\n\nSampling: [y, α, μ, ψ]\n\n\n\naz.plot_ppc(const_prior_pred, group=\"prior\", figsize=(12, 6));\n\n\n\n\n\nwith const_zi_unpooled_model:\n    const_idata = pm.sample(target_accept=0.9, idata_kwargs={\"log_likelihood\": True})\n    pm.sample_posterior_predictive(const_idata, extend_inferencedata=True)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [μ, α, ψ]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 6 seconds.\nSampling: [y]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:05&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:03&lt;00:00]\n    \n    \n\n\n\naz.plot_trace(const_idata, var_names=[\"μ\"], compact=False);\n\n\n\n\n\nplot_posterior_predictive_checks(const_idata);\n\n\n\n\n\naz.loo(const_idata)\n\nComputed from 4000 posterior samples and 130 observations log-likelihood matrix.\n\n         Estimate       SE\nelpd_loo  -439.75     9.32\np_loo       11.94        -\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.5]   (good)      130  100.0%\n (0.5, 0.7]   (ok)          0    0.0%\n   (0.7, 1]   (bad)         0    0.0%\n   (1, Inf)   (very bad)    0    0.0%\n\n\n\naz.plot_forest(const_idata, var_names=[\"μ_\"], combined=True, figsize=(12, 6));\n\n\n\n\n\n\nCode\nsns.set_theme(style=\"white\")\n\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\nf, ax = plt.subplots(figsize=(11, 9))\n\ncomparison_df, mask = make_comparison(const_idata, const_idx, const_labels)\n\nax = sns.heatmap(\n    comparison_df,\n    cmap=cmap,\n    mask=mask,\n    annot=True,\n    center=0,\n    square=True,\n    linewidths=0.5,\n    cbar_kws={\"shrink\": 0.5},\n    annot_kws={\"fontsize\": 8},\n).set(title=\"Constructor points expected over rival\");"
  },
  {
    "objectID": "posts/fantasy-gp-mcmc/fantasy-gp-mcmc.html#conclusion",
    "href": "posts/fantasy-gp-mcmc/fantasy-gp-mcmc.html#conclusion",
    "title": "Modeling Formula 1 Fantasy GP points with PyMC",
    "section": "8 Conclusion",
    "text": "8 Conclusion\nNone of the results here are that surprising, but it’s nonetheless useful to confirm our intuition for who’s a good pick and who isn’t. That said, the estimates should be taken with more than a grain of salt - the model we used is quite rudimentary (for one, it doesn’t account for performance trends as the season progresses), and modeling the summed fantasy points directly leads to a lot of detail getting obscured and left out.\n\n%load_ext watermark\n%watermark -n -u -v -iv\n\nThe watermark extension is already loaded. To reload it, use:\n  %reload_ext watermark\nLast updated: Mon Sep 04 2023\n\nPython implementation: CPython\nPython version       : 3.11.4\nIPython version      : 8.14.0\n\nsys       : 3.11.4 (main, Jun 28 2023, 19:51:46) [GCC]\nscipy     : 1.10.1\nrequests  : 2.31.0\nnumpy     : 1.25.2\npandas    : 2.1.0\narviz     : 0.16.1\npreliz    : 0.3.2\npymc      : 5.7.2\nseaborn   : 0.12.2\nmatplotlib: 3.7.2\njson      : 2.0.9\ntqdm      : 4.66.1"
  },
  {
    "objectID": "posts/fantasy-gp-mcmc/fantasy-gp-mcmc.html#footnotes",
    "href": "posts/fantasy-gp-mcmc/fantasy-gp-mcmc.html#footnotes",
    "title": "Modeling Formula 1 Fantasy GP points with PyMC",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nsee the intro to this for more details: https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-negative-binomial-regression.html↩︎\nsee this for a quick summary of model checking techniques: https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/pymc_overview.html#model-checking↩︎\nsee: https://bayesiancomputationbook.com/markdown/chp_02.html#effective-sample-size↩︎\nsee: https://bayesiancomputationbook.com/markdown/chp_02.html#potential-scale-reduction-factor-hat-r↩︎\nsee: https://bayesiancomputationbook.com/markdown/chp_02.html#monte-carlo-standard-error↩︎\nsee: https://bayesiancomputationbook.com/markdown/chp_02.html#understanding-your-predictions↩︎\nsee: https://bayesiancomputationbook.com/markdown/chp_02.html#fig-posterior-predictive-check-pu-values↩︎\nsee: https://oriolabrilpla.cat/en/blog/posts/2019/loo-pit-tutorial.html↩︎\nfor a detailed guide on interpreting the bpv plots see: https://bayesiancomputationbook.com/markdown/chp_02.html#fig-posterior-predictive-many-examples↩︎\nsee: https://sjster.github.io/introduction_to_computational_statistics/docs/Production/PyMC3.html#hdi↩︎\nsee: https://bayesiancomputationbook.com/markdown/chp_02.html#cross-validation-and-loo↩︎\nsee: https://bayesiancomputationbook.com/markdown/chp_02.html#model-comparison↩︎\nfor much more information on cross validation see: https://avehtari.github.io/modelselection/CV-FAQ.html#12_What_is_the_interpretation_of_ELPD__elpd_loo__elpd_diff↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m a software engineer at QuotaPath.\nI work mostly in Python, and occasionally in Rust and Scala.\nI live in Toronto with my dog."
  }
]